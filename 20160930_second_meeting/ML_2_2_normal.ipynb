{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "# Pythonで動かして学ぶ機械学習入門\n",
    "# 第二回 評判分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "## 0.前準備\n",
    "\n",
    "python versionの確認します。<br>\n",
    "current directoryとdata directoryの内容を確認します。<br>\n",
    "jupyter notebookではシェルコマンドの文頭に\"!\"をつけるとそのシェルコマンドをnotebook上で実行することができます。<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.4.3\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "902e411e-0466-4b01-a1df-f998ae9048c1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML_2_2_normal.ipynb \u001b[34mdata\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README \u001b[34mtokens\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dataの読み込みとモジュールのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyenvなどを用いているとpandasなどがimportできない場合がある。<br>\n",
    "その可能性の１つとしてlocale（国毎に異なる単位）の設定不足があり得るので、ここではそれを明示的に操作する。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your default locale is None\n",
      "Your locale is set as ja_JP.UTF-8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_locale():\n",
    "    default = os.environ.get('LC_ALL')\n",
    "    print( \"Your default locale is\", default )\n",
    "    if default is None:\n",
    "        os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')\n",
    "        print( \"Your locale is set as ja_JP.UTF-8\" )\n",
    "\n",
    "set_locale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うデータファイルのパスをpythonのリストとして取得します。<br>\n",
    "globはパス名を見つけたりparseしたりするモジュールです( http://docs.python.jp/3/library/glob.html )。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "DATA_PATH = \"./data/tokens/\"\n",
    "\n",
    "neg_files = glob.glob( \"{0}neg/*\".format(DATA_PATH) )\n",
    "pos_files = glob.glob( \"{0}pos/*\".format(DATA_PATH) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得したファイルパスの確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/tokens/neg/cv000_tok-9611.txt', './data/tokens/neg/cv001_tok-19324.txt']\n",
      "['./data/tokens/pos/cv000_tok-11609.txt', './data/tokens/pos/cv001_tok-10180.txt']\n"
     ]
    }
   ],
   "source": [
    "print(neg_files[0:2])\n",
    "print(pos_files[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込みテスト\n",
    "\n",
    "実際に文章を１つ読み込んでみて正しく読み込めているかを確認します。<br>\n",
    "テキストの読み込みはエンコーディングの問題などでエラーが生じやすいので、慣れるまでは根気強くdebugしましょう。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tristar / 1 : 30 / 1997 / r ( language , violence , dennis rodman ) cast : jean-claude van damme ; mickey rourke ; dennis rodman ; natacha lindinger ; paul freeman director : tsui hark screenplay : dan jakoby ; paul mones ripe with explosions , mass death and really weird hairdos , tsui hark's \" double team \" must be the result of a tipsy hollywood power lunch that decided jean-claude van damme needs another notch on his bad movie-bedpost and nba superstar dennis rodman should have an acting career . actually , in \" double team , \" neither's performance is all that bad . i've always been the one critic to defend van damme -- he possesses a high charisma level that some genre stars ( namely steven seagal ) never aim for ; it's just that he's never made a movie so exuberantly witty since 1994's \" timecop . \" and rodman . . . well , he's pretty much rodman . he's extremely colorful , and therefore he pretty much fits his role to a t , even if the role is that of an ex-cia weapons expert . it's the story that needs some major work . van damme plays counter-terrorist operative jack quinn , who teams up with arms dealer yaz ( rodman ) to rub out deadly gangster stavros ( mickey rourke , all beefy and weird-looking ) in an antwerp amusement park . the job is botched when stavros' son gets killed in the gunfire , and quinn is taken off to an island known as \" the colony \" -- a think tank for soldiers \" too valuable to kill \" but \" too dangerous to set free . \" quinn escapes and tries to make it back home to his pregnant wife ( natacha lindinger ) , but stavros is out for revenge and kidnaps her . so , what's a kickboxing mercenary to do ? quinn looks up yaz and the two travel to rome so they can rescue the woman , kill stavros , save the world and do whatever else the screenplay requires them to do . with crazy , often eye-popping camera work by peter pau and rodman's lite brite locks , \" double team \" should be a mildly enjoyable guilty pleasure . but too much tries to happen in each frame , and the result is a movie that leaves you exhausted rather than exhilarated . the numerous action scenes are loud and headache-inducing and the frenetic pacing never slows down enough for us to care about what's going on in the movie . and much of what's going on is just wacky . there's a whole segment devoted to net-surfing monks that i have yet to figure out . and the climax finds quinn going head-to-head with a tiger in the roman coliseum while yaz circles them on a motorcycle , trying to avoid running over land mines and hold on to quinn's baby boy ( who's in a bomb equipped basket ) -- all this while stavros watches shirtless from the bleachers . did i mention \" double team \" is strange ? when it all comes down , this is just another rarely entertaining formula killathon , albeit one that feels no need to indulge in gratuitous profanity . rodman juices things up with his blatantly vibrant screen persona , though , leading up to a stunt where he kicks an opponent between the legs . but we didn't need \" double team \" to tell us he could do that , did we ? 1997 jamie peck e-mail : <a href= \" mailto : jpeck1@gl . umbc . edu \" >jpeck1@gl . umbc . edu</a> visit the reel deal online : <a href= \" http : //www . gl . umbc . edu/~jpeck1/ \" >http : //www . gl . umbc . edu/~jpeck1/</a> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(neg_files[0], 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うモジュールの情報をまとめておきます。<br>\n",
    "- matplotlib : グラフなどの描写する\n",
    "- pandas : dataframeでデータを扱い、集計や統計量算出などがすぐ実行できる\n",
    "- collections : pythonで扱えるデータの型を提供する\n",
    "- sys : ファイルサイズ取得などのシステム上の操作を行う\n",
    "- numpy : 行列などの数学的オブジェクトを扱う\n",
    "- sklearn.feature_extraction.DictVectorizer : 辞書データをベクトルの形に変換する\n",
    "- sklearnのモデル : SVM, NB, RF\n",
    "- sklearn.grid_search : パラメタの最適な組み合わせ見つける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigramを作成する関数を定義します。<br>\n",
    "スペース区切りで単語を抽出し、その数をカウントする単純な関数となります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unigram(file_paths):\n",
    "    result = []\n",
    "    for file in file_paths:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = line.strip().split()\n",
    "                count_dicts = collections.Counter(words)\n",
    "                result.append( dict(count_dicts) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数を用いて、negative と positive 両方で unigram を作成します。<br>\n",
    "得られた2つのリストを合わせてモデルのインプットとします。\n",
    "リストの結合は \"+\" で実施できます。<br>\n",
    "negative と positive は各700文ずつありますが、そのうちいくつを使うかをここで指定します。初期設定では全てのデータを使うことになっていますが、後の過程で memory 不足になるようでしたらこの数を減らしてください。<br>\n",
    "\n",
    "ipython notebookでは %% をつけることでmagic commands ( https://ipython.org/ipython-doc/3/interactive/magics.html ) という便利なコマンドを実行できます。ここでは処理にかかる時間をセルに表示するコマンドを使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 459 ms, sys: 131 ms, total: 590 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NUM = 700\n",
    "\n",
    "unigrams_data = get_unigram(neg_files[:DATA_NUM]) + get_unigram(pos_files[:DATA_NUM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたunigram_dataを確認してみます。単語の出現数がカウントされていることが確認できます。<br>\n",
    "合わせてそのデータサイズも確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'superstar': 1, 'world': 1, ';': 6, 'much': 4, 'circles': 1, 'for': 4, 'each': 1, 'frame': 1, 'over': 1, 'antwerp': 1, '>http': 1, 'headache-inducing': 1, 'teams': 1, 'weird-looking': 1, 'whole': 1, 'acting': 1, 'nba': 1, 'climax': 1, 'tipsy': 1, 'known': 1, 'leading': 1, 'save': 1, '\"': 22, 'actually': 1, 'aim': 1, 'valuable': 1, ')': 6, 'out': 3, 'lite': 1, \"rodman's\": 1, 'loud': 1, 'camera': 1, 'work': 2, 'made': 1, 'hollywood': 1, 'tries': 2, 'from': 1, 'taken': 1, 'running': 1, 'though': 1, 'r': 1, 'the': 20, 'kicks': 1, 'by': 1, 'entertaining': 1, 'think': 1, \"i've\": 1, 'natacha': 2, 'mildly': 1, 'cast': 1, \"1994's\": 1, \"quinn's\": 1, 'rome': 1, 'care': 1, 'possesses': 1, 'them': 2, 'just': 3, '(': 6, 'result': 2, 'deadly': 1, 'needs': 2, 'tsui': 2, \"there's\": 1, 'quinn': 5, 'rub': 1, 'some': 2, 'two': 1, 'mines': 1, 'jpeck1@gl': 1, 'indulge': 1, 'jean-claude': 2, 'action': 1, 'wacky': 1, 'critic': 1, 'exuberantly': 1, 'if': 1, 'edu</a>': 1, 'timecop': 1, 'movie-bedpost': 1, 'travel': 1, 'colony': 1, 'finds': 1, 'lindinger': 2, 'gunfire': 1, 'umbc': 4, 'power': 1, 'pretty': 2, ':': 9, 'seagal': 1, 'i': 2, 'even': 1, 'brite': 1, 'stunt': 1, 'that': 10, 'team': 5, 'pleasure': 1, 'mickey': 2, 'decided': 1, '<a': 2, 'counter-terrorist': 1, 'mones': 1, 'down': 2, 'rourke': 2, 'another': 2, 'else': 1, 'land': 1, 'equipped': 1, 'need': 2, 'devoted': 1, 'all': 4, \"who's\": 1, \"stavros'\": 1, 'pau': 1, 'double': 5, 'back': 1, 'yaz': 3, \"he's\": 3, 'net-surfing': 1, 'they': 1, 'trying': 1, 'when': 2, 'notch': 1, 'watches': 1, 'weapons': 1, 'crazy': 1, 'plays': 1, '30': 1, 'operative': 1, 'defend': 1, 'you': 1, 'reel': 1, 'roman': 1, 'legs': 1, 'eye-popping': 1, 'stavros': 4, 'soldiers': 1, 'leaves': 1, 'witty': 1, 'requires': 1, 'killed': 1, 'so': 3, 'than': 1, 'set': 1, 'bad': 2, 't': 1, 'must': 1, 'shirtless': 1, 'level': 1, 'amusement': 1, 'damme': 4, 'screen': 1, 'in': 8, 'avoid': 1, 'edu/~jpeck1/</a>': 1, 'opponent': 1, 'major': 1, 'deal': 1, 'violence': 1, \"what's\": 3, 'dennis': 3, 'can': 1, 'looks': 1, 'are': 1, ',': 25, 'pregnant': 1, 'tell': 1, 'us': 2, 'scenes': 1, 'juices': 1, 'edu/~jpeck1/': 1, 'rarely': 1, 'coliseum': 1, 'mailto': 1, \"didn't\": 1, 'baby': 1, '?': 3, 'gl': 2, 'of': 3, 'island': 1, 'his': 4, 'jamie': 1, 'high': 1, 'off': 1, 'an': 5, 'job': 1, 'as': 1, 'do': 4, 'story': 1, 'performance': 1, 'happen': 1, 'been': 1, 'charisma': 1, 'hairdos': 1, '/': 3, 'a': 13, 'always': 1, 'boy': 1, 'guilty': 1, 'it': 2, 'weird': 1, 'have': 2, 'tiger': 1, 'could': 1, 'slows': 1, 'motorcycle': 1, 'paul': 2, 'on': 5, 'make': 1, 'director': 1, \"hark's\": 1, 'he': 4, 'locks': 1, 'mass': 1, 'figure': 1, 'expert': 1, 'hark': 1, 'with': 5, 'often': 1, 'van': 4, 'wife': 1, 'beefy': 1, 'free': 1, 'dan': 1, 'yet': 1, 'since': 1, 'killathon': 1, 'up': 4, \"neither's\": 1, 'bleachers': 1, 'gratuitous': 1, 'but': 4, 'colorful': 1, 'genre': 1, 'jakoby': 1, 'never': 3, 'be': 2, 'mention': 1, 'dangerous': 1, 'profanity': 1, 'exhilarated': 1, 'role': 2, 'basket': 1, 'we': 2, 'e-mail': 1, 'vibrant': 1, 'too': 3, 'movie': 3, 'numerous': 1, 'where': 1, '1': 1, 'gets': 1, 'gangster': 1, 'href=': 2, 'really': 1, 'one': 2, 'rather': 1, 'son': 1, 'revenge': 1, 'while': 2, 'visit': 1, 'persona': 1, 'enough': 1, 'tank': 1, 'and': 17, \"it's\": 2, 'who': 1, 'home': 1, 'dealer': 1, 'strange': 1, 'formula': 1, 'extremely': 1, 'segment': 1, 'escapes': 1, 'career': 1, 'blatantly': 1, 'is': 9, 'http': 1, 'ripe': 1, 'monks': 1, 'park': 1, 'botched': 1, 'no': 1, 'this': 2, 'namely': 1, 'woman': 1, 'did': 2, 'feels': 1, '>jpeck1@gl': 1, '--': 3, 'therefore': 1, 'edu': 1, 'pacing': 1, 'explosions': 1, 'kidnaps': 1, 'should': 2, 'death': 1, 'language': 1, '//www': 2, 'peck': 1, 'fits': 1, 'ex-cia': 1, 'tristar': 1, 'head-to-head': 1, 'kickboxing': 1, 'rodman': 7, 'mercenary': 1, 'to': 20, 'rescue': 1, 'going': 3, 'between': 1, 'whatever': 1, '.': 31, 'her': 1, 'arms': 1, 'freeman': 1, 'kill': 2, '1997': 2, 'exhausted': 1, 'jack': 1, 'frenetic': 1, 'enjoyable': 1, 'steven': 1, 'stars': 1, 'comes': 1, 'things': 1, 'online': 1, 'hold': 1, 'well': 1, 'about': 1, 'albeit': 1, 'lunch': 1, 'peter': 1, 'screenplay': 2, 'bomb': 1}\n",
      "data size : 0.011264 [MB]\n"
     ]
    }
   ],
   "source": [
    "print( unigrams_data[0] )\n",
    "print( \"data size :\", sys.getsizeof(unigrams_data) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたデータを扱いやすい行列の形にします。<br>\n",
    "各行が１つのレビューテキストで、各列が単語、要素がその単語の出現数というデータを作成します。<br>\n",
    "ここでは scikit-learn で準備されている DictVectorizer という関数を使うことでそれが簡単に実行できます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 983 ms, sys: 36.4 ms, total: 1.02 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = DictVectorizer()\n",
    "feature_vectors_csr = vec.fit_transform( unigrams_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したデータを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1400x44219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 496525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列の全成分は 60,000,000 要素くらいありますが、このうちのほとんどは 0 で 0 以外の値が入っているのは500,000程度です。<br>\n",
    "この CSR matrix　というのはこのような疎行列をの 0 でない成分だけを保持する賢いものになっています。<br>\n",
    "\n",
    "一方で 0 の成分を陽に保って普通の行列としてデータを保持することも可能です。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data dimension : (1400, 44219)\n",
      "[  0.   0.  22. ...,   0.   0.   0.]\n",
      "data size : 495.252912 [MB]\n"
     ]
    }
   ],
   "source": [
    "feature_vectors = vec.fit_transform( unigrams_data ).toarray()\n",
    "print( \"input data dimension :\", feature_vectors.shape )\n",
    "print( feature_vectors[0] )\n",
    "print( \"data size :\", sys.getsizeof(feature_vectors) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらはデータが非常に大きくなっていますが、これは 0 という成分を陽に保持しているためです。<br>\n",
    "この段階で memory error が生じる場合は一度 kernel を　restart して DATA_NUM の数を減らしてください。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ラベルデータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うデータセットは全てに negative → 0, positive → 1 というラベルが振られています。<br>\n",
    "特徴ベクトルはnegative sample 700文とpositive sample 700文を合わせて作ったものなので、0が700個と1が700個並んでいるベクトルを作成すればよいことになります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.r_[np.tile(0, DATA_NUM), np.tile(1, DATA_NUM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n"
     ]
    }
   ],
   "source": [
    "print( labels[0], labels[DATA_NUM-1], labels[DATA_NUM], labels[2*DATA_NUM-1]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.学習用データとテスト用データの作成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文の記述によれば、データを偏りがないように3分割に分け、three fold cross validationで評価しています。<br>\n",
    "ここでは乱数を生成して、データを3等分することで同様の状況を再現することにします。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7789)\n",
    "\n",
    "shuffle_order = np.random.choice( 2*DATA_NUM, 2*DATA_NUM, replace=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 1400\n",
      "first 10 elements : [1235 1232  910  162  343 1160  221  545 1112 1322]\n"
     ]
    }
   ],
   "source": [
    "print( \"length :\", len(shuffle_order) )\n",
    "print( \"first 10 elements :\", shuffle_order[0:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの偏りが生じていないかを確認します。<br>\n",
    "明らかに偏りが生じてしまった場合は乱数のseedを設定し直します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one third of the lengh : 466\n",
      "# of '1' in 1st set : 227\n",
      "# of '1' in 2nd set : 233\n",
      "# of '1' in 3rd set : 240\n"
     ]
    }
   ],
   "source": [
    "one_third_size = int( 2*DATA_NUM / 3. )\n",
    "print( \"one third of the lengh :\", one_third_size )\n",
    "\n",
    "print( \"# of '1' in 1st set :\", np.sum( labels[ shuffle_order[:one_third_size] ]  ) )\n",
    "print( \"# of '1' in 2nd set :\", np.sum( labels[ shuffle_order[one_third_size:2*one_third_size] ]  ) )\n",
    "print( \"# of '1' in 3rd set :\", np.sum( labels[ shuffle_order[2*one_third_size:] ]  ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.モデルを学習して精度を検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に必要な関数を定義します。<br>\n",
    "ここではモデルとして{Support Vector Machine, Naive Bayes, Random Forest}を用います。<br>\n",
    "モデルの性能測定は予測と答えが一致する数をカウントして正答率を求めることで実施します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def N_splitter(seq, N):\n",
    "    avg = len(seq) / float(N)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    \n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(features, labels, method='SVM', parameters=None):\n",
    "    ### set the model\n",
    "    if method == 'SVM':\n",
    "        model = svm.SVC()\n",
    "    elif method == 'NB':\n",
    "        model = naive_bayes.GaussianNB()\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        print(\"Set method as SVM (for Support vector machine), NB (for Naive Bayes) or RF (Random Forest)\")\n",
    "    ### set parameters if exists\n",
    "    if parameters:\n",
    "        model.set_params(**parameters)\n",
    "    ### train the model\n",
    "    model.fit( features, labels )\n",
    "    ### return the trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, features):\n",
    "    predictions = model.predict( features )\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(predictions, labels):\n",
    "    data_num = len(labels)\n",
    "    correct_num = np.sum( predictions == labels )\n",
    "    return data_num, correct_num\n",
    "\n",
    "def cross_validate(n_folds, feature_vectors, labels, shuffle_order, method='SVM', parameters=None):\n",
    "    result_test_num = []\n",
    "    result_correct_num = []\n",
    "    \n",
    "    n_splits = N_splitter( range(2*DATA_NUM), n_folds )\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print( \"Executing {0}th set...\".format(i+1) )\n",
    "        test_elems = shuffle_order[ n_splits[i] ]\n",
    "        train_elems = np.array([])\n",
    "        train_set = n_splits[ np.arange(n_folds) !=i ]\n",
    "        for j in train_set:\n",
    "            train_elems = np.r_[ train_elems, shuffle_order[j] ]\n",
    "        train_elems = train_elems.astype(np.integer)\n",
    "\n",
    "        # train\n",
    "        model = train_model( feature_vectors[train_elems], labels[train_elems], method, parameters )\n",
    "        # predict\n",
    "        predictions = predict( model, feature_vectors[test_elems] )\n",
    "        # evaluate\n",
    "        test_num, correct_num = evaluate_model( predictions, labels[test_elems] )\n",
    "        result_test_num.append( test_num )\n",
    "        result_correct_num.append( correct_num )\n",
    "    \n",
    "    return result_test_num, result_correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記関数はCross validationの数を変数として設定できるように作ってあります。<br>\n",
    "今回は 3-folds で分析を行います。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備ができたのでここでモデルを学習してその精度を確認してみましょう！<br>\n",
    "とりあえず何も考えずに準備した Bag Of Words をインプットにしてモデルを学習し、その精度を確認してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 14.3 s, sys: 238 ms, total: 14.6 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision 63.2 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision\", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**！！！具体的なテキスト確認を入れる！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.パラメタチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchでパラメタをチューニングすることでどの程度精度が上がるかを確認してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 56s, sys: 4.03 s, total: 5min\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision 79.5 %\n",
      "CPU times: user 13.9 s, sys: 146 ms, total: 14 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = clf.best_params_\n",
    "\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=params)\n",
    "\n",
    "print( \"average precision\", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度が15%程度も向上しました！機械学習においてモデルのパラメタチューニングが非常に重要であることが伺えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.簡単な特徴量変換による効果の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文に記載してあるように、Bag Of Words のカウント数を全て1にしてみることで精度にどのような変化が生じるかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_vectors_csr.data[ feature_vectors_csr.data > 0 ] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換したデータを用いて同様に学習プロセスを実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 15 s, sys: 151 ms, total: 15.2 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision 49.1 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision\", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんと精度がだいぶ落ちてしまいました。<br>\n",
    "しかも現在の問題設定は 0 か 1 を判別するものなので、ランダムに判別するモデルでも 50% 程度になります。<br>\n",
    "それと同程度ということは、そもそもモデルの学習が上手くいっていないのではないかということが疑われます。<br>\n",
    "そのことを検証してみるために、もう一度パラメタチューニングを実施してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 59s, sys: 3.1 s, total: 5min 2s\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 15 s, sys: 204 ms, total: 15.2 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision 82.7 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision\", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメタを調整することで高い精度を発揮することが分かりました！<br>\n",
    "この精度は論文に記載されているものとほぼ同程度のものとなっています。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.SVM以外のモデルを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes は sparse matrix 型のインプットを受け付けないので、numpy arrayとして作成したデータを入れなければなりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision 62.3 %\n",
      "CPU times: user 4.26 s, sys: 4.25 s, total: 8.51 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors, labels, shuffle_order, method='NB')\n",
    "print( \"average precision\", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest も実行してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision 65.8 %\n",
      "CPU times: user 2.87 s, sys: 1.07 s, total: 3.94 s\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors, labels, shuffle_order, method='RF')\n",
    "print( \"average precision\", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの結果はパラメタチューニングをしていないものなので、興味がある方はパラメタによって結果がどう変わるかを調べてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
