{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "import scipy.sparse as sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>LDAを使ってみよう</h2>\n",
    "\n",
    "LDAとはトピックモデルの手法の一つで、Pythonでは以下のようなライブラリでLDAを使用することができます。\n",
    "* scikit-learn (0.17以降）\n",
    "* lda\n",
    "* gensim\n",
    "\n",
    "ちなみに、LDAというと、Linear Discriminant Analysis(線形判別分析）と思う人もいるので、注意（実際、scikit-learnでLDAは線形判別分析と表しており、トピックモデルのLDAはLatentDirichletAllocationという名前）\n",
    "\n",
    "今回はlda_datasetフォルダにある2つのJSONファイルを使用します。これらはカメリオで収集した2016/10/21の記事をランダムに1000件サンプリングし、形態素解析した結果ですが、少しだけ異なる処理で2種類作成しました。document_word_data.jsonは名詞を全て含んだもの、document_word_data_pnoun.jsonはproper noun、つまり、固有名詞のみのデータです。これら2つを比較するのも面白いです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LDAのインプットデータを作ろう</h2>\n",
    "\n",
    "ここでのゴールは、文書ID x 単語の行列（成分は出現頻度）を作ることです。ここで注意しなければならないのは、この行列は凄まじくスパースである、つまり、ほとんどの成分が0であるということです。0であるということをデータ上に保存するのはもったいないので、0以外の成分のみをデータとして持っておきたいときに役立つのがスパース行列です。\n",
    "\n",
    "いろいろな表現方法がありますが、ここでは、List of List (LIL）という方法でデータを保存します。\n",
    "LILはとても簡単です。\n",
    "\n",
    "例えば、以下のようなAという行列があるとき、\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left[\n",
    "    \\begin{array}{rrr}\n",
    "      0 & 2 & 0 & 0 \\\\\n",
    "      1 & 0 & 0 & 1 \\\\\n",
    "      0 & 0 & 0 & 1\n",
    "    \\end{array}\n",
    "  \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "LILは、ゼロではない成分の行と列、その中の値を保存することで、上記の行列の情報を保存します。\n",
    "なので、上の$A$をLIL形式で表現すると、\n",
    "\\begin{equation*}\n",
    "(1, 2) = 2 ~~ (1行目2列目の成分が2)\\\\\n",
    "(2,1 ) = 1 ~~ (2行目1列目の成分が1)\\\\\n",
    "(2,4 ) = 1 ~~ (2行目4列目の成分が1)\\\\\n",
    "(3,4 ) = 1 ~~ (3行目4列目の成分が1)\\\\\n",
    "\\end{equation*}\n",
    "となります。\n",
    "\n",
    "では、LDAのインプットデータを作る場合も同様に考えられて、行に文書のインデックス、縦に単語のインデックスをとって、値があるところだけをLIL形式でデータを保存すれば良いということになります。\n",
    "\n",
    "さて、そこまでやってみましょう。まずデータを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./dataset/document_word_data.json\", \"r\") as f:\n",
    "    doc_data= json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print(\"Total Documents: \", len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この読み込んだjsonデータはこんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\", \".join(doc_data['715'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを作るために、全ての単語のリストを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_vocab = []\n",
    "for doc_idx in all_doc_index:\n",
    "    all_vocab += doc_data[doc_idx]\n",
    "\n",
    "#重複を消すためにsetしてlistにする\n",
    "all_vocab = list(set(all_vocab))\n",
    "vocab_num = len(all_vocab)\n",
    "print(\"Vocablary Number: \", vocab_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIL形式の行列を定義します。ScipyにはLIL形式でデータを保存するための機能がありますので、そちらを使います（ScipyはLIL以外のスパース行列のフォーマットをサポートしていますが）。\n",
    "\n",
    "ここからは学習データと学習後に適用するデータ（ここでは便宜的にテストデータ）に分けましょう。\n",
    "LDAは教師あり機械学習ではないので、ここでのテストデータの役割は学習済みのモデルに当てはめるデモ用という位置付けです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_doc_index_ar = np.array(list(all_doc_index))\n",
    "\n",
    "train_portion = 0.7\n",
    "train_num = int(len(all_doc_index_ar) * train_portion)\n",
    "\n",
    "np.random.shuffle(all_doc_index_ar)\n",
    "train_doc_index = all_doc_index_ar[:train_num]\n",
    "test_dox_index = all_doc_index_ar[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先にからっぽのスパース行列を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_A = sparse.lil_matrix((len(train_doc_index), len(all_vocab)), dtype = np.int)\n",
    "test_A = sparse.lil_matrix((len(test_dox_index), len(all_vocab)), dtype = np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ListからNumpyのArrayに直します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab_ar = np.array(all_vocab)\n",
    "train_doc_index_ar = np.array(train_doc_index)\n",
    "test_doc_index_ar = np.array(test_dox_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スパース行列に成分を入れていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#学習用\n",
    "train_total_elements_num = 0\n",
    "for i in xrange(len(train_doc_index)):\n",
    "    doc_idx = train_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        train_A[i, word_idx] = row_data[word]\n",
    "        train_total_elements_num += 1\n",
    "print(\"Train total elements num :\", train_total_elements_num)\n",
    "\n",
    "\n",
    "#テスト用\n",
    "test_total_elements_num = 0\n",
    "for i in xrange(len(test_dox_index)):\n",
    "    doc_idx = test_dox_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        test_A[i, word_idx] = row_data[word]\n",
    "        test_total_elements_num += 1\n",
    "print(\"Test total elements num :\", test_total_elements_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にLDAを適用してみよう (Scikit-learnを使った例）\n",
    "\n",
    "ここではsikit-learnの例を示します。scikit-learnのLDAはオンライン変分ベイズ法という推定方法を用いています。オンラインと名前が付いているので勘が良い方ならお気づきだと思いますが、SGDと同じように部分的にフィット(partial fit)させて、捨てるという形での推定が可能、つまり追加で学習がしやすいというところが特徴です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20, \n",
    "                                doc_topic_prior= 0.001,\n",
    "                               topic_word_prior=0.5,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.fit(train_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずトピック x 単語を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ /model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html　より\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([all_vocab_ar[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列側も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data = model1.transform(train_A)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのLDAはどうやら正規化されていないため、正規化した上で、1つ目の文書がどのトピックから来ている単語が多いのかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = doc_topic_data/doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):\n",
    "    print(\"Topic #%d: probality: %f\" % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当てはまりの度合いを測る指標の1つとして対数尤度(Loglikelihood)があります。できるだけ0に近いほどよく当てはまっていることになりますが、「X以上あれば精度が良い」と言えるような絶対的な指標ではなく、ハイパーパラメーターを変えた時などの相対的な比較に使うものだという点を気をつけてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglikelihood = model1.score(test_A)\n",
    "ppl = model1.perplexity(test_A)\n",
    "print(\"対数尤度: \", loglikelihood)\n",
    "print(\"Perplexity: \", ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに当てはめてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_doc_topic_data = model1.transform(test_A)\n",
    "normalize_test_doc_topic_data = test_doc_topic_data/test_doc_topic_data.sum(axis=1, keepdims=True)\n",
    "for topic_idx, prob in enumerate(normalize_test_doc_topic_data[0]):\n",
    "    print(\"Topic #%d: probality: %f\" % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LDAを適用してみよう (ldaパッケージを使った場合)</h2> \n",
    "\n",
    "ldaパッケージはより高速なGibbs samplingという手法を使って推定するパッケージです。ただし、これはオンライン学習はできない、つまりバッチ処理しかできません。また、変分ベイズ法による推定とGibbs samplingのどちらが精度が良いかというのはわかりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.fit(train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([all_vocab_ar[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も精度として対数尤度を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.loglikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data2 = model2.transform(train_A)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print(\"Topic #%d: probality: %f\" % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）ディリクレ分布の挙動</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 10.0\n",
    "K = 6\n",
    "sampled_probs = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs):\n",
    "    print(\"サイコロ %d面  確率: %.2f\"%(i+1, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）潜在ディリクレ分配法　少しばかり数式を使った解説</h2>\n",
    "<p>\n",
    "ある1つの文書データdの生成過程を数式で書くと,<br><br>\n",
    "文書データdの単語の個数を$N_d$とします。\n",
    "<br>\n",
    "<br>\n",
    "<h4>1. トピックの箱の選ばれやすさを決める</h4>\n",
    "トピックの確率分布はディリクレ分布に従うものとします。つまり、$\\theta_d \\sim Dir(\\alpha)$ です。この式の意味は、どのトピックの箱がどの程度選ばれるかが$\\theta_d$でハイパーパラメーターが$\\alpha$ということになり、$\\alpha$が1の時、どのトピックの箱が選ばれやすいかは完全にランダムになります。\n",
    "<br>\n",
    "<br>\n",
    "次に各単語について、以下の手順を想定します。\n",
    "<h4>2. トピックの箱を決める</h4>\n",
    "トピックの箱を決めるのにカテゴリ分布（多項分布）を想定します（ディリクレ分布とカテゴリ分布は共役だったことを思い出してください）。つまり、$z_{dn} \\sim Category(\\theta_d)$　を仮定します（$z_{dn}$はトピックの箱の番号だと思ってください）\n",
    "\n",
    "<h4>3. 単語カードを引く</h4>\n",
    "トピックの箱を決めた時に、そのトピックの箱から単語カード$w_n$を引く確率を$P(w_{dn} | \\phi_{z_n})$ ($n = 1,2, 3, \\cdots,  N_d$)と書きます。そして、この$P(w_dn | \\phi_{z_{dn}})$を多項分布、$\\phi_{z_dn}$をディリクレ分布で表現します。つまり、\n",
    "\\begin{equation*}\n",
    "\\phi_{z_{dn}} \\sim   Dir(\\beta)\n",
    "\\\\\n",
    "w_{dn} \\sim Category(\\phi_{z_{dn}} )\n",
    "\\end{equation*}\n",
    "となります。\n",
    "<br>\n",
    "<br>\n",
    "上記を組みわせて文書データdの生成仮定を以下のように書くことができます。\n",
    "\n",
    "\\begin{equation*}\n",
    "P(d | \\alpha, \\bf \\beta ) = \\int P(\\theta_d | \\alpha) \\prod_{n=1}^{N_d}P(z_{dn} |\\theta_d) P(w_{dn} | \\phi_{z_{dn}}) P(\\phi_{z_{dn}} | \\beta)d\\theta_d\n",
    "\\end{equation*}\n",
    "そして、M個の文書データ全体を$D$とすると以下のようになります。\n",
    "\\begin{equation*}\n",
    "P(D | \\alpha, \\bf \\beta ) = \\prod_{d=1}^{M}\\int P(\\theta_d | \\alpha) \\prod_{n=1}^{N_d}P(z_{dn} |\\theta_d) P(w_{dn} | \\phi_{z_{dn}}) P(\\phi_{z_{dn}} |\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若干、デフォルメしてありますが、上記を模したものが以下です。\n",
    "まず、以下のようにサッカー、音楽、経済の3つのトピックがあるものと仮定します。そして、それぞれのトピックに対し、どの単語がどの程度でやすいのかをディリクレ分布よりサンプリングします。その結果を「phi_トピック名」という配列に入れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_soccer = [\"サッカー\", \"本田圭佑\", \"セリエA\", \"日本代表\", \"香川真司\"]\n",
    "topic_music = [\"音楽\", \"ライブ\", \"ギター\", \"コンサート\", \"最高\", \"武道館\"]\n",
    "topic_econ = [\"経済\", \"株価\", \"日本企業\", \"スタートアップ\", \"Fintech\", \"マーケティング\"]\n",
    "topics = [topic_soccer, topic_music, topic_econ]\n",
    "\n",
    "beta = 0.5\n",
    "phi_soccer = np.random.dirichlet([beta for i in range(len(topic_soccer))])\n",
    "phi_music = np.random.dirichlet([beta for i in range(len(topic_music))])\n",
    "phi_econ = np.random.dirichlet([beta for i in range(len(topic_econ))])\n",
    "\n",
    "phi = [phi_soccer, phi_music, phi_econ]\n",
    "\n",
    "print(\"サッカートピックのPhi: \", phi_soccer)\n",
    "print(\"音楽トピックのPhi: \", phi_music)\n",
    "print(\"経済トピックのPhi: \", phi_econ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 10  #単語数\n",
    "alpha = 0.5 #トピック分布のハイパーパラメータ\n",
    "K = 3 #トピック数\n",
    "theta = np.random.dirichlet([alpha for i in range(3)])\n",
    "print(\"Theta :\", theta)\n",
    "\n",
    "generate_doc = []\n",
    "\n",
    "for i in range(N):\n",
    "    selected_topic_ar = np.random.multinomial(1.0, theta)\n",
    "    selected_topic_idx = np.where(selected_topic_ar==1)[0][0]\n",
    "    \n",
    "    \n",
    "    selected_word_ar = np.random.multinomial(1, phi[selected_topic_idx])\n",
    "    selected_word_idx = np.where(selected_word_ar==1)[0][0]\n",
    "    selected_word = topics[selected_topic_idx][selected_word_idx]\n",
    "    generate_doc.append(selected_word)\n",
    "    print(\"i = \", i, \"Selected topic:\", selected_topic_idx, \"Selected word:\", selected_word)\n",
    "\n",
    "    \n",
    "print('Finally, generated document is \"', \" \".join(generate_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような生成プロセスを経て、我々が目にしている文書が得られていると仮定するわけです。<br>\n",
    "さて、ではLDAで推定したいものは何でしょうか？それは、各トピックと$\\theta$と$\\phi$とかなのですが、その推定方法は、主に2つあります。一つは変分ベイズ\n",
    "法による推定、もう一つはGibbs samplingによる推定です。興味がある方は調べてみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
