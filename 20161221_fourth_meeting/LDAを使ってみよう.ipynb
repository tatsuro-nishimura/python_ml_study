{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import lda\n",
    "import scipy.sparse as sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>LDAを使ってみよう</h2>\n",
    "\n",
    "LDAとはトピックモデルの手法の一つで、Pythonでは以下のようなライブラリでLDAを使用することができます。\n",
    "* scikit-learn (0.17以降）\n",
    "* lda\n",
    "* gensim\n",
    "\n",
    "ちなみに、LDAというと、Linear Discriminant Analysis(線形判別分析）と思う人もいるので、注意（実際、scikit-learnでLDAは線形判別分析と表しており、トピックモデルのLDAはLatentDirichletAllocationという名前）\n",
    "\n",
    "今回はlda_datasetフォルダにある2つのJSONファイルを使用します。これらはカメリオで収集した2016/10/21の記事をランダムに1000件サンプリングし、形態素解析した結果ですが、少しだけ異なる処理で2種類作成しました。document_word_data.jsonは名詞を全て含んだもの、document_word_data_pnoun.jsonはproper noun、つまり、固有名詞のみのデータです。これら2つを比較するのも面白いです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LDAのインプットデータを作ろう</h2>\n",
    "\n",
    "ここでのゴールは、文書ID x 単語の行列（成分は出現頻度）を作ることです。ここで注意しなければならないのは、この行列は凄まじくスパースである、つまり、ほとんどの成分が0であるということです。0であるということをデータ上に保存するのはもったいないので、0以外の成分のみをデータとして持っておきたいときに役立つのがスパース行列です。\n",
    "\n",
    "いろいろな表現方法がありますが、ここでは、List of List (LIL）という方法でデータを保存します。\n",
    "LILはとても簡単です。\n",
    "\n",
    "例えば、以下のようなAという行列があるとき、\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\left[\n",
    "    \\begin{array}{rrr}\n",
    "      0 & 2 & 0 & 0 \\\\\n",
    "      1 & 0 & 0 & 1 \\\\\n",
    "      0 & 0 & 0 & 1\n",
    "    \\end{array}\n",
    "  \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "LILは、ゼロではない成分の行と列、その中の値を保存することで、上記の行列の情報を保存します。\n",
    "なので、上の$A$をLIL形式で表現すると、\n",
    "\\begin{equation*}\n",
    "(1, 2) = 2 ~~ (1行目2列目の成分が2)\\\\\n",
    "(2,1 ) = 1 ~~ (2行目1列目の成分が1)\\\\\n",
    "(2,4 ) = 1 ~~ (2行目4列目の成分が1)\\\\\n",
    "(3,4 ) = 1 ~~ (3行目4列目の成分が1)\\\\\n",
    "\\end{equation*}\n",
    "となります。\n",
    "\n",
    "では、LDAのインプットデータを作る場合も同様に考えられて、行に文書のインデックス、縦に単語のインデックスをとって、値があるところだけをLIL形式でデータを保存すれば良いということになります。\n",
    "\n",
    "さて、そこまでやってみましょう。まずデータを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents:  1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"./dataset/document_word_data.json\", \"r\") as f:\n",
    "    doc_data= json.load(f)\n",
    "\n",
    "all_doc_index = doc_data.keys()\n",
    "print(\"Total Documents: \", len(all_doc_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この読み込んだjsonデータはこんな感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ママ, 子供, 健康, づくり, 新た, ライフスタイル, 提案, ママ, マルシェ, 府, 府, 市, さまざま, 家族, ら, 日, もん, 商品, ほか, ハロ, 子供, 商品, プレゼント, 各日, 人, 会場, 木, ぬくもり, 木, 子供, づくり, 会, 木, 日, 午後, 時, 今年度, 森林, 林業, 木材, 大使, ミス, 日本, みどり, 帆, 南, さん, 府, 木材, 会, 湯川, 昌子, さん, ら, 女性, 人, 参加, スギ, ヒノキ, 木材, 放出, 健康, 効果, 木材, こと, 女性, 入場, 無料, 文化, 園, 入園, 料, 大人, 円, 円'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(doc_data['715'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のインデックスを作るために、全ての単語のリストを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocablary Number:  8709\n"
     ]
    }
   ],
   "source": [
    "all_vocab = []\n",
    "for doc_idx in all_doc_index:\n",
    "    all_vocab += doc_data[doc_idx]\n",
    "\n",
    "#重複を消すためにsetしてlistにする\n",
    "all_vocab = list(set(all_vocab))\n",
    "vocab_num = len(all_vocab)\n",
    "print(\"Vocablary Number: \", vocab_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIL形式の行列を定義します。ScipyにはLIL形式でデータを保存するための機能がありますので、そちらを使います（ScipyはLIL以外のスパース行列のフォーマットをサポートしていますが）。\n",
    "\n",
    "ここからは学習データと学習後に適用するデータ（ここでは便宜的にテストデータ）に分けましょう。\n",
    "LDAは教師あり機械学習ではないので、ここでのテストデータの役割は学習済みのモデルに当てはめるデモ用という位置付けです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_doc_index_ar = np.array(list(all_doc_index))\n",
    "\n",
    "train_portion = 0.7\n",
    "train_num = int(len(all_doc_index_ar) * train_portion)\n",
    "\n",
    "np.random.shuffle(all_doc_index_ar)\n",
    "train_doc_index = all_doc_index_ar[:train_num]\n",
    "test_dox_index = all_doc_index_ar[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先にからっぽのスパース行列を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_A = sparse.lil_matrix((len(train_doc_index), len(all_vocab)), dtype = np.int)\n",
    "test_A = sparse.lil_matrix((len(test_dox_index), len(all_vocab)), dtype = np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ListからNumpyのArrayに直します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vocab_ar = np.array(all_vocab)\n",
    "train_doc_index_ar = np.array(train_doc_index)\n",
    "test_doc_index_ar = np.array(test_dox_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スパース行列に成分を入れていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total elements num : 33609\n",
      "Test total elements num : 14435\n"
     ]
    }
   ],
   "source": [
    "#学習用\n",
    "train_total_elements_num = 0\n",
    "for i in xrange(len(train_doc_index)):\n",
    "    doc_idx = train_doc_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        train_A[i, word_idx] = row_data[word]\n",
    "        train_total_elements_num += 1\n",
    "print(\"Train total elements num :\", train_total_elements_num)\n",
    "\n",
    "\n",
    "#テスト用\n",
    "test_total_elements_num = 0\n",
    "for i in xrange(len(test_dox_index)):\n",
    "    doc_idx = test_dox_index[i]\n",
    "    row_data = Counter(doc_data[doc_idx])\n",
    "    \n",
    "    for word in row_data.keys():\n",
    "        word_idx = np.where(all_vocab_ar == word)[0][0]\n",
    "        test_A[i, word_idx] = row_data[word]\n",
    "        test_total_elements_num += 1\n",
    "print(\"Test total elements num :\", test_total_elements_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際にLDAを適用してみよう (Scikit-learnを使った例）\n",
    "\n",
    "ここではsikit-learnの例を示します。scikit-learnのLDAはオンライン変分ベイズ法という推定方法を用いています。オンラインと名前が付いているので勘が良い方ならお気づきだと思いますが、SGDと同じように部分的にフィット(partial fit)させて、捨てるという形での推定が可能、つまり追加で学習がしやすいというところが特徴です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = LatentDirichletAllocation(n_topics=20, \n",
    "                                doc_topic_prior= 0.001,\n",
    "                               topic_word_prior=0.5,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=0.001,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=20, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=0.5, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずトピック x 単語を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_components = model1.components_ /model1.components_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "棒 ブラ 大川 ホック 始末 はさみ ブック カラダ バンド イグアイン ミラン がま口 たわし 左利き セリエ 交差 つ折り 人差し指 フェルト メイド\n",
      "\n",
      "Topic #1:\n",
      "ダンベル ジム マシン 器具 島根 スタイラス マグネット キタンクラブ 入り口 徳島 ネオジウム 収束 兵庫 マイクロリッジ 南東 ウェイト チェストプレス 栃 日吉津 ウェイトベルト\n",
      "\n",
      "Topic #2:\n",
      "日 月 こと 年 人 の よう 円 時 日本 ため 方 分 東京 女性 さん ん イベント 情報 市\n",
      "\n",
      "Topic #3:\n",
      "和南 東大和 チェア アッフル マカロン 清瀬 帝京 債 東大 実 準決勝 ボンド 円建て 債券 ペカ ジュ ジェイクライプ アイドリング 周波 アイアンスヘ\n",
      "\n",
      "Topic #4:\n",
      "なつ トリガ 本土 セフ 東京大学 俊彦 崩れ わら 沖 将希 ガブガブ かさ キムキョンジャ 水嶋 スロアプリ まさし スリップ 干支 お母様 ルフフェア\n",
      "\n",
      "Topic #5:\n",
      "ポグバ ユナイテッド ラプソリュ ランコム 倦怠期 ナウ 加入 センシュアル エンブレム 振替 マット マルカ 今夏 ユベントス オレンジ フェミニン 仕掛け マッチ 意地 ゲンダイ\n",
      "\n",
      "Topic #6:\n",
      "マリオピカチュウ 朝比奈 南方 マスコット 海南 マリオ グッズ ピカチュウ オリジナルグッズ 前期 ぬいぐるみ 広州 マリオピカチュウスペシャル ちょ サロペット アモイ チャイナ ヒゲ 動揺 エア\n",
      "\n",
      "Topic #7:\n",
      "海津 亮介 ヴィツェル ゼニト ヒルナンデス パン ごっこ マスク シアワセ クロワッサン ヴァ 夏希 導 モンハン ドラクエコラボ クソワロタ ふみ パズドラ ロケ かあさん\n",
      "\n",
      "Topic #8:\n",
      "ブラシ ブラッシング 毛 抜け毛 ツヤ バルセロナ 就寝 埃 汚れ ナイロン 入浴 弾力 ただ メンズ クッション 加減 巧 水洗 バルサ タイミング\n",
      "\n",
      "Topic #9:\n",
      "曽根崎新地 成徳 檜山 少数 ト 可否 てん リック しそ ヘルス サン ブイヨン ニヨン 安易 東北大 おっぱい お母様 ドイル 浩文 エフ\n",
      "\n",
      "Topic #10:\n",
      "栗 小布施 モンブラン ハウス 宿 小布施堂 山梨 テラス 庵 本店 あん ジャングル 栗子 ホタル チェンマイ 液晶 フィルム 焼き ナポリ ぶどう\n",
      "\n",
      "Topic #11:\n",
      "ヨシヒコ 核兵器 点滴 導師 佐多 政治 抑止 戦士 けい 延滞 メロ 杖 バッシア 工業団地 カプラン 弟分 ドジ 暴君 サラゴナ ヒサ\n",
      "\n",
      "Topic #12:\n",
      "ブラント バイエルン リストアップ ビデオ 朗報 放出 深夜 大山 来夏 ユリ ロベリ リベリ 決断 撤廃 テンション チェコ ユベントス プライス フィンランド シティ\n",
      "\n",
      "Topic #13:\n",
      "おじ ジバニャン トムニャン 妖怪 ウォッチ コチラ アオキ ハンサム クスリ ファミマ ポインテッドトゥ コラム シルエット カジュアル つきあい まちがい エルメス 泣き こなし アンクル\n",
      "\n",
      "Topic #14:\n",
      "サワコ 佐和子 放射 典子 波 いきさつ 光学 エッセイスト 工学 おれ 原子 照射 受像 ランタン ロックバンド 海流 おまえ しのぎ たん 地帯\n",
      "\n",
      "Topic #15:\n",
      "ノルディック 柄 ニット 占い トス 入札 デニム パンツ カラオケ マイク シャツ 居 ドッキング ティペット ワイド カミラ 易 スゲェ ちゃんこ め\n",
      "\n",
      "Topic #16:\n",
      "俵山 きち ゆず ピット ツルハ 審 損害 札幌 温泉 パンフレット 添 シリア 先月 むら あて エプロン むかご 原告 ジェット機 潘\n",
      "\n",
      "Topic #17:\n",
      "族 ダンソン ダンスィングフィッソン コント リズムダンソン 名作 タップ リズム グラタン 吸引 決心 年寄り 岩波書店 ウィンド ゾンビ 尿意 ディレクションスタジオ アイリッシュ 改ざん スポニチ\n",
      "\n",
      "Topic #18:\n",
      "ドラ 潜在 たま ゲリラ マッサン オムツ ノマダン タマゴ サファ ダンジョン 友情 ラグオデ ハロウィンダンジョン 合成 ゼウスディオス レベルアップ ハロウィンガチャ レアガチャ スペダン テクダン\n",
      "\n",
      "Topic #19:\n",
      "桜 千本 ヒド しげ エニックス 槇 未來 渡 スクウェア 大正 憑 京 影 安売り 支出 コミカライズ カラオケ ヤング 宙 楽曲\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html　より\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(normalize_components):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([all_vocab_ar[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列側も見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.92562309e-06,   4.92562309e-06,   6.32897504e-01, ...,\n",
       "          4.92562309e-06,   4.92562309e-06,   4.92562309e-06],\n",
       "       [  4.75737393e-05,   4.75737393e-05,   9.99096099e-01, ...,\n",
       "          4.75737393e-05,   4.75737393e-05,   4.75737393e-05],\n",
       "       [  3.56887937e-05,   3.56887937e-05,   5.40818685e-01, ...,\n",
       "          3.56887937e-05,   3.56887937e-05,   3.56887937e-05],\n",
       "       ..., \n",
       "       [  1.85116623e-05,   1.85116623e-05,   9.99648278e-01, ...,\n",
       "          1.85116623e-05,   1.85116623e-05,   1.85116623e-05],\n",
       "       [  4.44404942e-06,   4.44404942e-06,   9.99915563e-01, ...,\n",
       "          4.44404942e-06,   4.44404942e-06,   4.44404942e-06],\n",
       "       [  2.32450023e-05,   2.32450023e-05,   9.99558345e-01, ...,\n",
       "          2.32450023e-05,   2.32450023e-05,   2.32450023e-05]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_data = model1.transform(train_A)\n",
    "doc_topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのLDAはどうやら正規化されていないため、正規化した上で、1つ目の文書がどのトピックから来ている単語が多いのかを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_doc_topic_data = doc_topic_data/doc_topic_data.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000005\n",
      "Topic #1: probality: 0.000005\n",
      "Topic #2: probality: 0.632898\n",
      "Topic #3: probality: 0.000005\n",
      "Topic #4: probality: 0.000005\n",
      "Topic #5: probality: 0.000005\n",
      "Topic #6: probality: 0.000005\n",
      "Topic #7: probality: 0.000005\n",
      "Topic #8: probality: 0.000005\n",
      "Topic #9: probality: 0.000005\n",
      "Topic #10: probality: 0.000005\n",
      "Topic #11: probality: 0.000005\n",
      "Topic #12: probality: 0.000005\n",
      "Topic #13: probality: 0.000005\n",
      "Topic #14: probality: 0.000005\n",
      "Topic #15: probality: 0.367014\n",
      "Topic #16: probality: 0.000005\n",
      "Topic #17: probality: 0.000005\n",
      "Topic #18: probality: 0.000005\n",
      "Topic #19: probality: 0.000005\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, prob in enumerate(normalize_doc_topic_data[0]):\n",
    "    print(\"Topic #%d: probality: %f\" % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当てはまりの度合いを測る指標の1つとして対数尤度(Loglikelihood)があります。できるだけ0に近いほどよく当てはまっていることになりますが、「X以上あれば精度が良い」と言えるような絶対的な指標ではなく、ハイパーパラメーターを変えた時などの相対的な比較に使うものだという点を気をつけてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数尤度:  -682950.745332\n",
      "Perplexity:  181288115381.0\n"
     ]
    }
   ],
   "source": [
    "loglikelihood = model1.score(test_A)\n",
    "ppl = model1.perplexity(test_A)\n",
    "print(\"対数尤度: \", loglikelihood)\n",
    "print(\"Perplexity: \", ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに当てはめてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: probality: 0.000010\n",
      "Topic #1: probality: 0.000010\n",
      "Topic #2: probality: 0.877610\n",
      "Topic #3: probality: 0.000010\n",
      "Topic #4: probality: 0.000010\n",
      "Topic #5: probality: 0.000010\n",
      "Topic #6: probality: 0.000010\n",
      "Topic #7: probality: 0.122217\n",
      "Topic #8: probality: 0.000010\n",
      "Topic #9: probality: 0.000010\n",
      "Topic #10: probality: 0.000010\n",
      "Topic #11: probality: 0.000010\n",
      "Topic #12: probality: 0.000010\n",
      "Topic #13: probality: 0.000010\n",
      "Topic #14: probality: 0.000010\n",
      "Topic #15: probality: 0.000010\n",
      "Topic #16: probality: 0.000010\n",
      "Topic #17: probality: 0.000010\n",
      "Topic #18: probality: 0.000010\n",
      "Topic #19: probality: 0.000010\n"
     ]
    }
   ],
   "source": [
    "test_doc_topic_data = model1.transform(test_A)\n",
    "normalize_test_doc_topic_data = test_doc_topic_data/test_doc_topic_data.sum(axis=1, keepdims=True)\n",
    "for topic_idx, prob in enumerate(normalize_test_doc_topic_data[0]):\n",
    "    print(\"Topic #%d: probality: %f\" % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LDAを適用してみよう (ldaパッケージを使った場合)</h2> \n",
    "\n",
    "ldaパッケージはより高速なGibbs samplingという手法を使って推定するパッケージです。ただし、これはオンライン学習はできない、つまりバッチ処理しかできません。また、変分ベイズ法による推定とGibbs samplingのどちらが精度が良いかというのはわかりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = lda.LDA(n_topics=20, n_iter=1500, random_state=1, alpha=0.5, eta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 700\n",
      "INFO:lda:vocab_size: 8709\n",
      "INFO:lda:n_words: 59539\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "WARNING:lda:all zero column in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -680872\n",
      "INFO:lda:<10> log likelihood: -536233\n",
      "INFO:lda:<20> log likelihood: -504567\n",
      "INFO:lda:<30> log likelihood: -498322\n",
      "INFO:lda:<40> log likelihood: -497583\n",
      "INFO:lda:<50> log likelihood: -497217\n",
      "INFO:lda:<60> log likelihood: -497090\n",
      "INFO:lda:<70> log likelihood: -497645\n",
      "INFO:lda:<80> log likelihood: -496475\n",
      "INFO:lda:<90> log likelihood: -496952\n",
      "INFO:lda:<100> log likelihood: -497135\n",
      "INFO:lda:<110> log likelihood: -496887\n",
      "INFO:lda:<120> log likelihood: -497318\n",
      "INFO:lda:<130> log likelihood: -497490\n",
      "INFO:lda:<140> log likelihood: -496499\n",
      "INFO:lda:<150> log likelihood: -497342\n",
      "INFO:lda:<160> log likelihood: -496959\n",
      "INFO:lda:<170> log likelihood: -497451\n",
      "INFO:lda:<180> log likelihood: -496253\n",
      "INFO:lda:<190> log likelihood: -496619\n",
      "INFO:lda:<200> log likelihood: -497028\n",
      "INFO:lda:<210> log likelihood: -496397\n",
      "INFO:lda:<220> log likelihood: -496464\n",
      "INFO:lda:<230> log likelihood: -496635\n",
      "INFO:lda:<240> log likelihood: -496480\n",
      "INFO:lda:<250> log likelihood: -497096\n",
      "INFO:lda:<260> log likelihood: -496627\n",
      "INFO:lda:<270> log likelihood: -496582\n",
      "INFO:lda:<280> log likelihood: -496908\n",
      "INFO:lda:<290> log likelihood: -496721\n",
      "INFO:lda:<300> log likelihood: -496109\n",
      "INFO:lda:<310> log likelihood: -496579\n",
      "INFO:lda:<320> log likelihood: -496669\n",
      "INFO:lda:<330> log likelihood: -496453\n",
      "INFO:lda:<340> log likelihood: -496333\n",
      "INFO:lda:<350> log likelihood: -495501\n",
      "INFO:lda:<360> log likelihood: -496788\n",
      "INFO:lda:<370> log likelihood: -497041\n",
      "INFO:lda:<380> log likelihood: -496927\n",
      "INFO:lda:<390> log likelihood: -496611\n",
      "INFO:lda:<400> log likelihood: -496784\n",
      "INFO:lda:<410> log likelihood: -496842\n",
      "INFO:lda:<420> log likelihood: -496878\n",
      "INFO:lda:<430> log likelihood: -496383\n",
      "INFO:lda:<440> log likelihood: -496591\n",
      "INFO:lda:<450> log likelihood: -495950\n",
      "INFO:lda:<460> log likelihood: -496962\n",
      "INFO:lda:<470> log likelihood: -497375\n",
      "INFO:lda:<480> log likelihood: -496612\n",
      "INFO:lda:<490> log likelihood: -497059\n",
      "INFO:lda:<500> log likelihood: -495697\n",
      "INFO:lda:<510> log likelihood: -496622\n",
      "INFO:lda:<520> log likelihood: -496479\n",
      "INFO:lda:<530> log likelihood: -496094\n",
      "INFO:lda:<540> log likelihood: -496357\n",
      "INFO:lda:<550> log likelihood: -496390\n",
      "INFO:lda:<560> log likelihood: -497003\n",
      "INFO:lda:<570> log likelihood: -496829\n",
      "INFO:lda:<580> log likelihood: -496920\n",
      "INFO:lda:<590> log likelihood: -496702\n",
      "INFO:lda:<600> log likelihood: -496641\n",
      "INFO:lda:<610> log likelihood: -496852\n",
      "INFO:lda:<620> log likelihood: -496604\n",
      "INFO:lda:<630> log likelihood: -496814\n",
      "INFO:lda:<640> log likelihood: -496558\n",
      "INFO:lda:<650> log likelihood: -496876\n",
      "INFO:lda:<660> log likelihood: -496139\n",
      "INFO:lda:<670> log likelihood: -495808\n",
      "INFO:lda:<680> log likelihood: -495827\n",
      "INFO:lda:<690> log likelihood: -496401\n",
      "INFO:lda:<700> log likelihood: -496851\n",
      "INFO:lda:<710> log likelihood: -496879\n",
      "INFO:lda:<720> log likelihood: -496821\n",
      "INFO:lda:<730> log likelihood: -496184\n",
      "INFO:lda:<740> log likelihood: -497194\n",
      "INFO:lda:<750> log likelihood: -496350\n",
      "INFO:lda:<760> log likelihood: -496603\n",
      "INFO:lda:<770> log likelihood: -496359\n",
      "INFO:lda:<780> log likelihood: -496021\n",
      "INFO:lda:<790> log likelihood: -496538\n",
      "INFO:lda:<800> log likelihood: -495924\n",
      "INFO:lda:<810> log likelihood: -496747\n",
      "INFO:lda:<820> log likelihood: -496310\n",
      "INFO:lda:<830> log likelihood: -496217\n",
      "INFO:lda:<840> log likelihood: -496235\n",
      "INFO:lda:<850> log likelihood: -495859\n",
      "INFO:lda:<860> log likelihood: -495981\n",
      "INFO:lda:<870> log likelihood: -495594\n",
      "INFO:lda:<880> log likelihood: -496315\n",
      "INFO:lda:<890> log likelihood: -496043\n",
      "INFO:lda:<900> log likelihood: -496340\n",
      "INFO:lda:<910> log likelihood: -496646\n",
      "INFO:lda:<920> log likelihood: -496088\n",
      "INFO:lda:<930> log likelihood: -495850\n",
      "INFO:lda:<940> log likelihood: -495943\n",
      "INFO:lda:<950> log likelihood: -495677\n",
      "INFO:lda:<960> log likelihood: -495740\n",
      "INFO:lda:<970> log likelihood: -496033\n",
      "INFO:lda:<980> log likelihood: -496258\n",
      "INFO:lda:<990> log likelihood: -496019\n",
      "INFO:lda:<1000> log likelihood: -495852\n",
      "INFO:lda:<1010> log likelihood: -496490\n",
      "INFO:lda:<1020> log likelihood: -495639\n",
      "INFO:lda:<1030> log likelihood: -496127\n",
      "INFO:lda:<1040> log likelihood: -495596\n",
      "INFO:lda:<1050> log likelihood: -495756\n",
      "INFO:lda:<1060> log likelihood: -496213\n",
      "INFO:lda:<1070> log likelihood: -496248\n",
      "INFO:lda:<1080> log likelihood: -496322\n",
      "INFO:lda:<1090> log likelihood: -495991\n",
      "INFO:lda:<1100> log likelihood: -495952\n",
      "INFO:lda:<1110> log likelihood: -496109\n",
      "INFO:lda:<1120> log likelihood: -496243\n",
      "INFO:lda:<1130> log likelihood: -496462\n",
      "INFO:lda:<1140> log likelihood: -495768\n",
      "INFO:lda:<1150> log likelihood: -495993\n",
      "INFO:lda:<1160> log likelihood: -496455\n",
      "INFO:lda:<1170> log likelihood: -495482\n",
      "INFO:lda:<1180> log likelihood: -495670\n",
      "INFO:lda:<1190> log likelihood: -495967\n",
      "INFO:lda:<1200> log likelihood: -495969\n",
      "INFO:lda:<1210> log likelihood: -495338\n",
      "INFO:lda:<1220> log likelihood: -495116\n",
      "INFO:lda:<1230> log likelihood: -496043\n",
      "INFO:lda:<1240> log likelihood: -496062\n",
      "INFO:lda:<1250> log likelihood: -495274\n",
      "INFO:lda:<1260> log likelihood: -496301\n",
      "INFO:lda:<1270> log likelihood: -496058\n",
      "INFO:lda:<1280> log likelihood: -495517\n",
      "INFO:lda:<1290> log likelihood: -495481\n",
      "INFO:lda:<1300> log likelihood: -496191\n",
      "INFO:lda:<1310> log likelihood: -495792\n",
      "INFO:lda:<1320> log likelihood: -496328\n",
      "INFO:lda:<1330> log likelihood: -495859\n",
      "INFO:lda:<1340> log likelihood: -496467\n",
      "INFO:lda:<1350> log likelihood: -496490\n",
      "INFO:lda:<1360> log likelihood: -496521\n",
      "INFO:lda:<1370> log likelihood: -495909\n",
      "INFO:lda:<1380> log likelihood: -496443\n",
      "INFO:lda:<1390> log likelihood: -496534\n",
      "INFO:lda:<1400> log likelihood: -496189\n",
      "INFO:lda:<1410> log likelihood: -495950\n",
      "INFO:lda:<1420> log likelihood: -495654\n",
      "INFO:lda:<1430> log likelihood: -496350\n",
      "INFO:lda:<1440> log likelihood: -496769\n",
      "INFO:lda:<1450> log likelihood: -495829\n",
      "INFO:lda:<1460> log likelihood: -495704\n",
      "INFO:lda:<1470> log likelihood: -496401\n",
      "INFO:lda:<1480> log likelihood: -496238\n",
      "INFO:lda:<1490> log likelihood: -496296\n",
      "INFO:lda:<1499> log likelihood: -496174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x10a6d3b38>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "バイト アルバイト トランプ 在住 入学 先 ミス 就活 合格 みなさん 休業 学業 コツ クリントン 夫妻 法 奨学 対処 挑戦 幼児\n",
      "\n",
      "Topic #1:\n",
      "栗 味噌 ジャクソン 明 小布施 味 弓子 ドラマ コラボ スカイ 大倉 塔 マンション ジョンソン 漬け 佐 モンブラン 夜 教室 剛\n",
      "\n",
      "Topic #2:\n",
      "企業 化 日本 こと 性 提供 支援 くしゃみ 向け 広告 協会 光 年 法人 ため 対応 ビジネス タイヤ システム 安全\n",
      "\n",
      "Topic #3:\n",
      "コナン 探偵 アニメ スタジオ 占い 江 仲 安室 アンジュルム バンド ビュッシュ 剛 新曲 昌 ドラム 暮 体制 アプリ ハロ 法律\n",
      "\n",
      "Topic #4:\n",
      "ゴジラ 曲 タイム ライン ホップ アルバム ヒップ バンド ホック ブック トラック シン 大作 ラップ ブラ 浮気 ミ ナナ ラブラブ プリント\n",
      "\n",
      "Topic #5:\n",
      "位 年 投手 戦 指名 ドラフト 優勝 日 回 出場 プロ 広島 期待 勝利 日本ハム 点 対戦 勝 手 決勝\n",
      "\n",
      "Topic #6:\n",
      "こと の 人 よう さん 年 ため 歳 ん もの 方 円 女性 たち そう これ 日本 日 価格 何\n",
      "\n",
      "Topic #7:\n",
      "市 倉吉 弱 岡山 午後 強 区 さ 揺れ 広島 気温 津波 ごろ 北栄 熊本 災害 兵庫 梨 分 湯\n",
      "\n",
      "Topic #8:\n",
      "アイテム 大人 柄 ニット バッグ ノルディック おじ 感 冬 海津 ブラシ 毛 カジュアル コチラ 亮介 パンツ 店 夏 ドレス プラス\n",
      "\n",
      "Topic #9:\n",
      "声優 所 捜査 士 死亡 動機 授業 巨人 志望 戸塚 平塚 望 ごろ 法 所属 役 古城 傷害 所内 卒業\n",
      "\n",
      "Topic #10:\n",
      "すみれ エイリアン グレイ ピット 同性愛 マッサン 人影 火 デジカメ スゴイ トレッサ 提案 先月 次回 このほど 先日 手 めちゃくちゃ 正午 リアル\n",
      "\n",
      "Topic #11:\n",
      "商品 サイズ さ 枚 こ す 価格 板 個 作り方 店 円 名 式 インテリア セ タイプ おしゃれ リア 横\n",
      "\n",
      "Topic #12:\n",
      "入場 新婦 ハノイ 土曜日 ドン サプライズ 携帯 レスリング 扉 払い戻し 全文 文春 ストライプ ロンドン 府 初日 アディダス 整体 キムタク ラブ\n",
      "\n",
      "Topic #13:\n",
      "巻 号 新刊 ブログ コミック クリック アイドル 原作 妖怪 十 勝 ジバニャン 俺 以内 アニメ くん 希 桜 姫 パン\n",
      "\n",
      "Topic #14:\n",
      "プラン モバイル デバイス 毎月 ウェブ カ月 フォン 十分 残り 歯 拡張 いくつ その他 大河 果実 ビタミン さ マッシュポテト 勢 医\n",
      "\n",
      "Topic #15:\n",
      "くん ユニセフ 太 イエメン 感染 停戦 保健 コレラ 国内 活動 子ども 川崎 たち 国 便 当局 全土 アクセス 民 把握\n",
      "\n",
      "Topic #16:\n",
      "円 億 市場 日 株 機 期 修正 年 ドル 営業 基準 月 取引 比 場合 平成 性 企業 株式\n",
      "\n",
      "Topic #17:\n",
      "ハウス 宿 木 マック 床 器具 ジム ジャングル 小売 チェンマイ 暖 モザイク ダンベル マシン ホタル 外 場所 城 列島 タイプ\n",
      "\n",
      "Topic #18:\n",
      "日 月 年 時 東京 イベント 分 情報 参加 会 市 こと 名 会場 円 氏 サイト 区 土 実施\n",
      "\n",
      "Topic #19:\n",
      "安倍 官 晋 政府 グラス ジャスティン 政権 仁 安保 佐 判 四 テレビ局 新入 法制 メダリスト 文庫 憲法 漢字 六\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_word = model2.topic_word_\n",
    "n_top_words = 20\n",
    "for topic_idx, topic in enumerate(topic_word):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([all_vocab_ar[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回も精度として対数尤度を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.loglikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書 x トピック行列を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_data2 = model2.transform(train_A)\n",
    "for topic_idx, prob in enumerate(doc_topic_data2[0]):\n",
    "    print(\"Topic #%d: probality: %f\" % (topic_idx, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）ディリクレ分布の挙動</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サイコロ 1面  確率: 0.00\n",
      "サイコロ 2面  確率: 0.81\n",
      "サイコロ 3面  確率: 0.02\n",
      "サイコロ 4面  確率: 0.00\n",
      "サイコロ 5面  確率: 0.17\n",
      "サイコロ 6面  確率: 0.00\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "K = 6\n",
    "sampled_probs = np.random.dirichlet([alpha for i in range(K)])\n",
    "for i, prob in enumerate(sampled_probs):\n",
    "    print(\"サイコロ %d面  確率: %.2f\"%(i+1, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>（参考）潜在ディリクレ分配法　少しばかり数式を使った解説</h2>\n",
    "<p>\n",
    "ある1つの文書データdの生成過程を数式で書くと,<br><br>\n",
    "文書データdの単語の個数を$N_d$とします。\n",
    "<br>\n",
    "<br>\n",
    "<h4>1. トピックの箱の選ばれやすさを決める</h4>\n",
    "トピックの確率分布はディリクレ分布に従うものとします。つまり、$\\theta_d \\sim Dir(\\alpha)$ です。この式の意味は、どのトピックの箱がどの程度選ばれるかが$\\theta_d$でハイパーパラメーターが$\\alpha$ということになり、$\\alpha$が1の時、どのトピックの箱が選ばれやすいかは完全にランダムになります。\n",
    "<br>\n",
    "<br>\n",
    "次に各単語について、以下の手順を想定します。\n",
    "<h4>2. トピックの箱を決める</h4>\n",
    "トピックの箱を決めるのにカテゴリ分布（多項分布）を想定します（ディリクレ分布とカテゴリ分布は共役だったことを思い出してください）。つまり、$z_{dn} \\sim Category(\\theta_d)$　を仮定します（$z_{dn}$はトピックの箱の番号だと思ってください）\n",
    "\n",
    "<h4>3. 単語カードを引く</h4>\n",
    "トピックの箱を決めた時に、そのトピックの箱から単語カード$w_n$を引く確率を$P(w_{dn} | \\phi_{z_n})$ ($n = 1,2, 3, \\cdots,  N_d$)と書きます。そして、この$P(w_dn | \\phi_{z_{dn}})$を多項分布、$\\phi_{z_dn}$をディリクレ分布で表現します。つまり、\n",
    "\\begin{equation*}\n",
    "\\phi_{z_{dn}} \\sim   Dir(\\beta)\n",
    "\\\\\n",
    "w_{dn} \\sim Category(\\phi_{z_{dn}} )\n",
    "\\end{equation*}\n",
    "となります。\n",
    "<br>\n",
    "<br>\n",
    "上記を組みわせて文書データdの生成仮定を以下のように書くことができます。\n",
    "\n",
    "\\begin{equation*}\n",
    "P(d | \\alpha, \\bf \\beta ) = \\int P(\\theta_d | \\alpha) \\prod_{n=1}^{N_d}P(z_{dn} |\\theta_d) P(w_{dn} | \\phi_{z_{dn}}) P(\\phi_{z_{dn}} | \\beta)d\\theta_d\n",
    "\\end{equation*}\n",
    "そして、M個の文書データ全体を$D$とすると以下のようになります。\n",
    "\\begin{equation*}\n",
    "P(D | \\alpha, \\bf \\beta ) = \\prod_{d=1}^{M}\\int P(\\theta_d | \\alpha) \\prod_{n=1}^{N_d}P(z_{dn} |\\theta_d) P(w_{dn} | \\phi_{z_{dn}}) P(\\phi_{z_{dn}} |\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若干、デフォルメしてありますが、上記を模したものが以下です。\n",
    "まず、以下のようにサッカー、音楽、経済の3つのトピックがあるものと仮定します。そして、それぞれのトピックに対し、どの単語がどの程度でやすいのかをディリクレ分布よりサンプリングします。その結果を「phi_トピック名」という配列に入れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_soccer = [\"サッカー\", \"本田圭佑\", \"セリエA\", \"日本代表\", \"香川真司\"]\n",
    "topic_music = [\"音楽\", \"ライブ\", \"ギター\", \"コンサート\", \"最高\", \"武道館\"]\n",
    "topic_econ = [\"経済\", \"株価\", \"日本企業\", \"スタートアップ\", \"Fintech\", \"マーケティング\"]\n",
    "topics = [topic_soccer, topic_music, topic_econ]\n",
    "\n",
    "beta = 0.5\n",
    "phi_soccer = np.random.dirichlet([beta for i in range(len(topic_soccer))])\n",
    "phi_music = np.random.dirichlet([beta for i in range(len(topic_music))])\n",
    "phi_econ = np.random.dirichlet([beta for i in range(len(topic_econ))])\n",
    "\n",
    "phi = [phi_soccer, phi_music, phi_econ]\n",
    "\n",
    "print(\"サッカートピックのPhi: \", phi_soccer)\n",
    "print(\"音楽トピックのPhi: \", phi_music)\n",
    "print(\"経済トピックのPhi: \", phi_econ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 10  #単語数\n",
    "alpha = 0.5 #トピック分布のハイパーパラメータ\n",
    "K = 3 #トピック数\n",
    "theta = np.random.dirichlet([alpha for i in range(3)])\n",
    "print(\"Theta :\", theta)\n",
    "\n",
    "generate_doc = []\n",
    "\n",
    "for i in range(N):\n",
    "    selected_topic_ar = np.random.multinomial(1.0, theta)\n",
    "    selected_topic_idx = np.where(selected_topic_ar==1)[0][0]\n",
    "    \n",
    "    \n",
    "    selected_word_ar = np.random.multinomial(1, phi[selected_topic_idx])\n",
    "    selected_word_idx = np.where(selected_word_ar==1)[0][0]\n",
    "    selected_word = topics[selected_topic_idx][selected_word_idx]\n",
    "    generate_doc.append(selected_word)\n",
    "    print(\"i = \", i, \"Selected topic:\", selected_topic_idx, \"Selected word:\", selected_word)\n",
    "\n",
    "    \n",
    "print('Finally, generated document is \"', \" \".join(generate_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような生成プロセスを経て、我々が目にしている文書が得られていると仮定するわけです。<br>\n",
    "さて、ではLDAで推定したいものは何でしょうか？それは、各トピックと$\\theta$と$\\phi$とかなのですが、その推定方法は、主に2つあります。一つは変分ベイズ\n",
    "法による推定、もう一つはGibbs samplingによる推定です。興味がある方は調べてみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
